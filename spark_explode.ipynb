{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.2.0-bin-hadoop3.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002D99AD7FCA0>\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\") \\\n",
    "                    .appName('My App') \\\n",
    "                    .getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|         Name|Salary|\n",
      "+-------------+------+\n",
      "|      Hemanth|250000|\n",
      "|          Hem|350000|\n",
      "|Hemanth Kumar|210000|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"Hemanth\", 250000), (\"Hem\", 350000), (\"Hemanth Kumar\", 210000)],[\"Name\",\"Salary\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subjects: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-------+-----------------------------------+\n",
      "|name   |subjects                           |\n",
      "+-------+-----------------------------------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
      "+-------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|name   |col               |\n",
      "+-------+------------------+\n",
      "|James  |[Java, Scala, C++]|\n",
      "|James  |[Spark, Java]     |\n",
      "|Michael|[Spark, Java, C++]|\n",
      "|Michael|[Spark, Java]     |\n",
      "|Robert |[CSharp, VB]      |\n",
      "|Robert |[Spark, Python]   |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.subjects)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------+------------------+\n",
      "|name   |subjects                           |explode_cal       |\n",
      "+-------+-----------------------------------+------------------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++]|\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Spark, Java]     |\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java]     |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB]      |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[Spark, Python]   |\n",
      "+-------+-----------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"explode_cal\",explode(df.subjects))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------+------------------+--------+\n",
      "|name   |subjects                           |explode_cal       |from_cal|\n",
      "+-------+-----------------------------------+------------------+--------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++]|Java    |\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++]|Scala   |\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++]|C++     |\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Spark, Java]     |Spark   |\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Spark, Java]     |Java    |\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++]|Spark   |\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++]|Java    |\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++]|C++     |\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java]     |Spark   |\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java]     |Java    |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB]      |CSharp  |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB]      |VB      |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[Spark, Python]   |Spark   |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[Spark, Python]   |Python  |\n",
      "+-------+-----------------------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"from_cal\",explode(df.explode_cal))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n",
      "|name   |flatten(subjects)              |\n",
      "+-------+-------------------------------+\n",
      "|James  |[Java, Scala, C++, Spark, Java]|\n",
      "|Michael|[Spark, Java, C++, Spark, Java]|\n",
      "|Robert |[CSharp, VB, Spark, Python]    |\n",
      "+-------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import flatten\n",
    "df.select(df.name,flatten(df.subjects)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------+------------------+--------+-------------------------------+\n",
      "|name   |subjects                           |explode_cal       |from_cal|allsub                         |\n",
      "+-------+-----------------------------------+------------------+--------+-------------------------------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++]|Java    |[Java, Scala, C++, Spark, Java]|\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++]|Scala   |[Java, Scala, C++, Spark, Java]|\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++]|C++     |[Java, Scala, C++, Spark, Java]|\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Spark, Java]     |Spark   |[Java, Scala, C++, Spark, Java]|\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|[Spark, Java]     |Java    |[Java, Scala, C++, Spark, Java]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++]|Spark   |[Spark, Java, C++, Spark, Java]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++]|Java    |[Spark, Java, C++, Spark, Java]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++]|C++     |[Spark, Java, C++, Spark, Java]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java]     |Spark   |[Spark, Java, C++, Spark, Java]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java]     |Java    |[Spark, Java, C++, Spark, Java]|\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB]      |CSharp  |[CSharp, VB, Spark, Python]    |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB]      |VB      |[CSharp, VB, Spark, Python]    |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[Spark, Python]   |Spark   |[CSharp, VB, Spark, Python]    |\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |[Spark, Python]   |Python  |[CSharp, VB, Spark, Python]    |\n",
      "+-------+-----------------------------------+------------------+--------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"allsub\",flatten(df.subjects))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  James|\n",
      "|Michael|\n",
      "| Robert|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-14e34c356c8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mgroup_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollect_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_user\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import collect_set\n",
    "rdd = spark.sparkContext.parallelize([Row(user='Bob', word='hello'),\n",
    "                                      Row(user='Bob', word='world'),\n",
    "                                      Row(user='Mary', word='Have'),\n",
    "                                      Row(user='Mary', word='a'),\n",
    "                                      Row(user='Mary', word='nice'),\n",
    "                                      Row(user='Mary', word='day')])\n",
    "df = spark.createDataFrame(rdd)\n",
    "group_user = df.groupBy('user').agg(collect_set('word').alias('words'))\n",
    "sat = group_user.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbe58ca63fe33f9eeae9e71d10368d2b4a57f2b1b395836210cc60d362c66949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
